{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4636949a-4a27-40a5-b675-cfb8bee8e986",
   "metadata": {},
   "source": [
    "# Metric Analysis with Full Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95d366c-0070-420a-9176-6323eeb13631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "2025-04-09 15:57:47,363 - ERROR - Error: The directory /Users/ptr/Documents/Projects/AnomaLLMy/src/metrics_analysis/analysis_results does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # Import numpy for handling potential NaN in plotting\n",
    "import logging # Use logging for better messages\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the path to the directory containing the analysis results files\n",
    "# Adjust this path as needed relative to where your notebook is located.\n",
    "# Example: If notebook is in the project root, and results are in analyzer/analysis_results\n",
    "# analysis_results_dir = './analyzer/analysis_results/'\n",
    "# Example: If notebook is inside the analyzer directory\n",
    "analysis_results_dir = './analysis_results/'\n",
    "# Or provide an absolute path:\n",
    "# analysis_results_dir = '/Users/ptr/Documents/Projects/AnomaLLMy/analyzer/analysis_results/'\n",
    "\n",
    "# --- Path Validation ---\n",
    "analysis_results_dir = os.path.abspath(analysis_results_dir) # Convert to absolute path\n",
    "if not os.path.isdir(analysis_results_dir):\n",
    "    logging.error(f\"Error: The directory {analysis_results_dir} does not exist.\")\n",
    "    # You might want to raise an error or exit here depending on the notebook context\n",
    "    # raise FileNotFoundError(f\"Directory not found: {analysis_results_dir}\")\n",
    "else:\n",
    "    logging.info(f\"Searching for analysis files in: {analysis_results_dir}\")\n",
    "\n",
    "    # --- Data Extraction ---\n",
    "    # Define lists to store the metrics\n",
    "    llm_names = []\n",
    "    response_times = []\n",
    "    chunk_times = []\n",
    "    response_rates = []\n",
    "    word_counts = []\n",
    "    char_counts = []\n",
    "    sentence_counts = []\n",
    "    avg_word_lengths = []\n",
    "    avg_sentence_lengths = []\n",
    "    vocab_richnesses = []\n",
    "\n",
    "    # Regular expressions to extract the metrics from the file content\n",
    "    # These match the keys printed by consolidate_metrics\n",
    "    metrics_regex = {\n",
    "        'Total Elapsed Time': r\"Total Elapsed Time:\\s*([\\d.]+)\",\n",
    "        'Average Response Rate': r\"Average Response Rate:\\s*([\\d.]+)\", # Renamed from 'Response Rate' to match output\n",
    "        'Total Word Count': r\"Total Word Count:\\s*(\\d+)\", # Renamed from 'Word Count'\n",
    "        'Total Character Count': r\"Total Character Count:\\s*(\\d+)\", # Renamed from 'Character Count'\n",
    "        'Total Sentence Count': r\"Total Sentence Count:\\s*(\\d+)\", # Renamed from 'Sentence Count'\n",
    "        'Average Word Length': r\"Average Word Length:\\s*([\\d.]+)\",\n",
    "        'Average Sentence Length': r\"Average Sentence Length:\\s*([\\d.]+)\",\n",
    "        'Average Vocabulary Richness': r\"Average Vocabulary Richness:\\s*([\\d.]+)\" # Renamed from 'Vocabulary Richness'\n",
    "        # 'Average Chunk Time' seems missing from consolidate_metrics output, removing for now\n",
    "        # If you add it back to consolidate_metrics, uncomment and adjust regex here:\n",
    "        # 'Average Chunk Time': r\"Average Chunk Time:\\s*([\\d.]+)\",\n",
    "    }\n",
    "    # Regex to extract the LLM model name from the header\n",
    "    model_name_regex = r\"Model Used:\\s*(.*)\"\n",
    "\n",
    "    # Loop through all text files in the directory\n",
    "    found_files = 0\n",
    "    processed_files = 0\n",
    "    for filename in os.listdir(analysis_results_dir):\n",
    "        # Process only files matching the expected analysis output format\n",
    "        if filename.endswith('.txt') and '_analysis_' in filename:\n",
    "            found_files += 1\n",
    "            file_path = os.path.join(analysis_results_dir, filename)\n",
    "            logging.info(f\"Processing file: {filename}\")\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                # 1. Extract LLM Name from content\n",
    "                model_match = re.search(model_name_regex, content)\n",
    "                if not model_match:\n",
    "                    logging.warning(f\"Could not find 'Model Used:' in file {filename}. Skipping.\")\n",
    "                    continue # Skip this file if model name is missing\n",
    "                llm_name = model_match.group(1).strip()\n",
    "\n",
    "                # 2. Extract Metrics from content\n",
    "                extracted_metrics = {}\n",
    "                metrics_found = True\n",
    "                for key, regex in metrics_regex.items():\n",
    "                    match = re.search(regex, content)\n",
    "                    if match:\n",
    "                        try:\n",
    "                            # Attempt to convert to float immediately\n",
    "                            extracted_metrics[key] = float(match.group(1))\n",
    "                        except ValueError:\n",
    "                             logging.warning(f\"Could not convert value for '{key}' to float in {filename}. Skipping file.\")\n",
    "                             metrics_found = False\n",
    "                             break # Stop processing metrics for this file\n",
    "                    else:\n",
    "                        logging.warning(f\"Could not find metric '{key}' in file {filename}. Skipping file.\")\n",
    "                        metrics_found = False\n",
    "                        break # Stop processing metrics for this file\n",
    "\n",
    "                # Only append if all required metrics were found and valid\n",
    "                if metrics_found:\n",
    "                    llm_names.append(llm_name)\n",
    "                    response_times.append(extracted_metrics.get('Total Elapsed Time', np.nan)) # Use np.nan for safety\n",
    "                    # chunk_times.append(extracted_metrics.get('Average Chunk Time', np.nan)) # If added back\n",
    "                    response_rates.append(extracted_metrics.get('Average Response Rate', np.nan))\n",
    "                    word_counts.append(extracted_metrics.get('Total Word Count', np.nan))\n",
    "                    char_counts.append(extracted_metrics.get('Total Character Count', np.nan))\n",
    "                    sentence_counts.append(extracted_metrics.get('Total Sentence Count', np.nan))\n",
    "                    avg_word_lengths.append(extracted_metrics.get('Average Word Length', np.nan))\n",
    "                    avg_sentence_lengths.append(extracted_metrics.get('Average Sentence Length', np.nan))\n",
    "                    vocab_richnesses.append(extracted_metrics.get('Average Vocabulary Richness', np.nan))\n",
    "                    processed_files += 1\n",
    "                else:\n",
    "                     logging.warning(f\"Skipped appending data for {filename} due to missing/invalid metrics.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing file {filename}: {e}\")\n",
    "                continue # Skip to next file on general error\n",
    "\n",
    "    logging.info(f\"Found {found_files} potential analysis files. Successfully processed {processed_files}.\")\n",
    "\n",
    "    # --- DataFrame Creation and Processing ---\n",
    "    if not llm_names:\n",
    "        logging.error(\"No valid data extracted. Cannot create DataFrame or plots.\")\n",
    "    else:\n",
    "        # Create a DataFrame\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'LLM': llm_names,\n",
    "            'Total Response Time': response_times,\n",
    "            # 'Average Chunk Time': chunk_times, # If added back\n",
    "            'Average Response Rate': response_rates,\n",
    "            'Total Word Count': word_counts,\n",
    "            'Total Character Count': char_counts,\n",
    "            'Total Sentence Count': sentence_counts,\n",
    "            'Average Word Length': avg_word_lengths,\n",
    "            'Average Sentence Length': avg_sentence_lengths,\n",
    "            'Average Vocabulary Richness': vocab_richnesses\n",
    "        })\n",
    "\n",
    "        # Convert columns to numeric (already attempted during extraction, but this handles potential NaNs)\n",
    "        for column in metrics_df.columns[1:]:\n",
    "            metrics_df[column] = pd.to_numeric(metrics_df[column], errors='coerce')\n",
    "\n",
    "        # Remove rows with any NaN values that might have occurred\n",
    "        original_rows = len(metrics_df)\n",
    "        metrics_df.dropna(inplace=True)\n",
    "        if len(metrics_df) < original_rows:\n",
    "             logging.warning(f\"Removed {original_rows - len(metrics_df)} rows with missing/invalid data after conversion.\")\n",
    "\n",
    "        # Aggregate metrics if multiple files used the same LLM (average)\n",
    "        if not metrics_df.empty:\n",
    "             logging.info(\"Aggregating metrics by LLM name (using mean)...\")\n",
    "             metrics_df = metrics_df.groupby('LLM', as_index=False).mean()\n",
    "\n",
    "             logging.info(\"Final DataFrame for plotting:\")\n",
    "             print(metrics_df) # Display the aggregated data\n",
    "             logging.info(\"Data types of the final DataFrame:\")\n",
    "             print(metrics_df.dtypes) # Display data types\n",
    "\n",
    "             # --- Plotting ---\n",
    "             def plot_metric(df, metric, color, ylabel, title, horizontal=False):\n",
    "                 # Ensure the DataFrame is not empty before plotting\n",
    "                 if df.empty:\n",
    "                     logging.warning(f\"Cannot plot '{title}': DataFrame is empty.\")\n",
    "                     return\n",
    "                 # Ensure the metric column exists\n",
    "                 if metric not in df.columns:\n",
    "                      logging.warning(f\"Cannot plot '{title}': Metric column '{metric}' not found in DataFrame.\")\n",
    "                      return\n",
    "\n",
    "                 # Sort the DataFrame by the specified metric in ascending order\n",
    "                 df_sorted = df.sort_values(by=metric, ascending=True)\n",
    "\n",
    "                 sorted_llm_names = df_sorted['LLM'].tolist()\n",
    "                 sorted_metric_values = df_sorted[metric].tolist()\n",
    "\n",
    "                 logging.info(f\"Plotting {title}: {len(sorted_llm_names)} models\")\n",
    "                 # print(list(zip(sorted_llm_names, sorted_metric_values))) # Optional: print sorted values\n",
    "\n",
    "                 plt.figure(figsize=(12, max(6, len(sorted_llm_names) * 0.5))) # Adjust height based on number of models for barh\n",
    "                 if horizontal:\n",
    "                     bars = plt.barh(sorted_llm_names, sorted_metric_values, color=color)\n",
    "                     plt.xlabel(ylabel)\n",
    "                     # Add value labels to the bars\n",
    "                     plt.bar_label(bars, fmt='%.2f', padding=3)\n",
    "                 else:\n",
    "                     bars = plt.bar(sorted_llm_names, sorted_metric_values, color=color)\n",
    "                     plt.ylabel(ylabel)\n",
    "                     plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "                     # Add value labels to the bars\n",
    "                     plt.bar_label(bars, fmt='%.2f', padding=3)\n",
    "\n",
    "                 plt.title(title)\n",
    "                 plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
    "                 plt.show()\n",
    "\n",
    "             # Plot each metric (adjust column names if needed based on final DataFrame)\n",
    "             plot_metric(metrics_df, 'Total Response Time', 'skyblue', 'Time (seconds)', 'Total Response Time per LLM (Lower is Faster)', horizontal=True)\n",
    "             # plot_metric(metrics_df, 'Average Chunk Time', 'lightgreen', 'Time (seconds)', 'Average Chunk Time per LLM (Sorted)', horizontal=True) # If added back\n",
    "             plot_metric(metrics_df, 'Average Response Rate', 'mediumpurple', 'Words per Second', 'Average Response Rate per LLM (Higher is Faster)', horizontal=True)\n",
    "             plot_metric(metrics_df, 'Total Word Count', 'lightcoral', 'Words', 'Total Word Count per LLM (Higher is More Verbose)', horizontal=True)\n",
    "             plot_metric(metrics_df, 'Total Character Count', 'gold', 'Characters', 'Total Character Count per LLM (Higher is More Verbose)', horizontal=True)\n",
    "             plot_metric(metrics_df, 'Total Sentence Count', 'lightseagreen', 'Sentences', 'Total Sentence Count per LLM (Higher is More Verbose)', horizontal=True)\n",
    "             plot_metric(metrics_df, 'Average Word Length', 'tan', 'Characters per Word', 'Average Word Length per LLM', horizontal=True)\n",
    "             plot_metric(metrics_df, 'Average Sentence Length', 'lightpink', 'Words per Sentence', 'Average Sentence Length per LLM', horizontal=True)\n",
    "             plot_metric(metrics_df, 'Average Vocabulary Richness', 'yellowgreen', 'Richness Score (Higher is More Diverse)', 'Average Vocabulary Richness per LLM', horizontal=True)\n",
    "\n",
    "        else:\n",
    "             logging.error(\"DataFrame is empty after processing and cleaning. No plots will be generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39ca03-e04f-49cd-80a9-c9cf3a055ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
